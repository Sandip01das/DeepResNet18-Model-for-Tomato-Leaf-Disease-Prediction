{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydis_zDtB2bL"
      },
      "outputs": [],
      "source": [
        "#importing necessary libraries\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import time\n",
        "\n",
        "#Datasets\n",
        "\n",
        "train_dir = '/content/drive/MyDrive/CropDiseaseDetection/Train'\n",
        "valid_dir = '/content/drive/MyDrive/CropDiseaseDetection/Valid'\n",
        "test_dir  = '/content/drive/MyDrive/CropDiseaseDetection/Test'\n",
        "\n",
        "#preprocessing\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(20),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "valid_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "train_dataset = ImageFolder(train_dir, transform=train_transforms)\n",
        "valid_dataset = ImageFolder(valid_dir, transform=valid_transforms)\n",
        "test_dataset  = ImageFolder(test_dir, transform=test_transforms)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "valid_loader = DataLoader(valid_dataset, batch_size=32, shuffle=False)\n",
        "test_loader  = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "class_names = train_dataset.classes\n",
        "print(f\"Classes: {class_names}\")\n",
        "class_names1 = test_dataset.classes\n",
        "print(f\"Classes 2:{class_names1}\")\n",
        "\n",
        "# Model\n",
        "model = models.resnet18(pretrained=True)\n",
        "for param in model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_ftrs, 256),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.4),\n",
        "    nn.Linear(256, len(class_names))\n",
        ")\n",
        "#model = model.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.fc.parameters(), lr=0.001)\n",
        "\n",
        "# Train Function\n",
        "def train_model(model, train_loader, valid_loader, criterion, optimizer, num_epochs=100):\n",
        "    since = time.time()\n",
        "    train_losses, valid_losses = [], []\n",
        "    train_accs, valid_accs = [], []\n",
        "    best_model_wts = model.state_dict()\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print('-' * 20)\n",
        "\n",
        "        for phase in ['train', 'valid']:\n",
        "            model.train() if phase == 'train' else model.eval()\n",
        "            dataloader = train_loader if phase == 'train' else valid_loader\n",
        "            running_loss, running_corrects = 0.0, 0\n",
        "\n",
        "            for inputs, labels in dataloader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs)\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            epoch_loss = running_loss / len(dataloader.dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloader.dataset)\n",
        "\n",
        "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}\")\n",
        "            if phase == 'train':\n",
        "                train_losses.append(epoch_loss)\n",
        "                train_accs.append(epoch_acc.item())\n",
        "            else:\n",
        "                valid_losses.append(epoch_loss)\n",
        "                valid_accs.append(epoch_acc.item())\n",
        "                if epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = model.state_dict()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f\"\\nTraining complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "    print(f\"Best Validation Accuracy: {best_acc:.4f}\")\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, train_losses, valid_losses, train_accs, valid_accs\n",
        "\n",
        "model, train_losses, valid_losses, train_accs, valid_accs = train_model(\n",
        "    model, train_loader, valid_loader, criterion, optimizer, num_epochs=100)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Evaluation\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()\n",
        "    preds, targets = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            _, predictions = torch.max(outputs, 1)\n",
        "            preds.extend(predictions.cpu().numpy())\n",
        "            targets.extend(labels.cpu().numpy())\n",
        "\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(targets, preds, target_names=class_names))\n",
        "    cm = confusion_matrix(targets, preds)\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(cm)\n",
        "    return preds, targets, cm\n",
        "\n",
        "preds, targets, cm = evaluate(model, test_loader)\n",
        "accuracy = np.mean(np.array(preds) == np.array(targets))\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Confusion Matrix Heatmap\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=class_names, yticklabels=class_names)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"Actual\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Precision, Recall, F1 Plot\n",
        "precision, recall, f1, _ = precision_recall_fscore_support(targets, preds, average=None)\n",
        "x = np.arange(len(class_names))\n",
        "width = 0.20\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(x - width, precision, width, label='Precision')\n",
        "plt.bar(x, recall, width, label='Recall')\n",
        "plt.bar(x + width, f1, width, label='F1 Score')\n",
        "plt.xticks(x, class_names, rotation=45)\n",
        "plt.ylabel('Score')\n",
        "plt.title('Precision, Recall, F1 Score per Class')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8tI-ZEqJCBOd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}